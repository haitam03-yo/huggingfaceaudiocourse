# Unit 1. Working with audio data

### **Audio Signals in Nature vs. Digital Representation**

1. **Natural Audio Signals**
   * **Continuous** : Sound waves in nature are **analog** and have infinite detail.
   * **Impossible to Capture Directly** : We can only approximate them using  **sampling** .
2. **Sampling Process (Time ‚Üí Amplitude)**
   * We **divide the continuous signal** into **small discrete time intervals** (samples).
   * Each sample represents the **amplitude** of the signal at that moment.
   * **Higher Sampling Rate = More Accurate Approximation**
     * Example: **44.1 kHz (CD quality)** means we take  **44,100 samples per second** .
     * More samples ‚Üí Closer to the original sound.
3. **Bit Depth (Accuracy of Each Sample‚Äôs Amplitude)**
   * Defines **how precisely we store the amplitude value** for each sample.
   * More bits = **More levels of amplitude** =  **Less quantization error (more precision)** .
     * **8-bit** ‚Üí 256 levels (low quality).
     * **16-bit** ‚Üí 65,536 levels (CD quality).
     * **24-bit** ‚Üí 16.7 million levels (studio quality).

---

### **üîπ From Time-Amplitude to Frequency-Amplitude (DFT & FFT)**

1. **Why Transform Audio into Frequency Domain?**
   * Time-domain representation (waveform) shows how sound varies over time.
   * But we often need to know **which frequencies are present** (like in music, speech, or noise).
2. **DFT (Discrete Fourier Transform) & FFT (Fast Fourier Transform)**
   * **DFT converts time-domain signals ‚Üí frequency-domain representation** .
   * FFT is just a **faster** version of DFT.
   * It decomposes the signal into  **sine waves of different frequencies & amplitudes** .
3. **Spectrogram: Visualizing Frequency Changes Over Time**
   * Spectrogram =  **Sequence of FFTs over small time windows** .
   * X-axis = **Time**
   * Y-axis = **Frequency**
   * Color = **Amplitude (intensity of frequencies)**

---

### **üîπ Key Takeaways**

‚úÖ **Sampling captures audio in discrete time steps (higher sampling = better approximation).**
‚úÖ **Bit depth controls amplitude precision (higher bit depth = more accurate sound).**
‚úÖ **DFT/FFT transforms audio into frequency domain (to analyze pitch, tone, etc.).**
‚úÖ **Spectrograms show frequency changes over time (for speech, music, and noise analysis).**


Sampling is the process of measuring the value of a continuous signal at fixed time steps. The sampled waveform is  *discrete* , since it contains a finite number of signal values at uniform intervals.


### ** Fast Fourier Transform (FFT) Explained**

**FFT (Fast Fourier Transform)** is an **efficient algorithm** used to compute the **Discrete Fourier Transform (DFT)** of a signal. It transforms a signal from the **time domain** (amplitude vs. time) to the **frequency domain** (amplitude vs. frequency).


### spectrogram

Since the spectrogram and the waveform are different views of the same data, it‚Äôs possible to turn the spectrogram back into the original waveform using the inverse STFT. However, this requires the phase information in addition to the amplitude information. If the spectrogram was generated by a machine learning model, it typically only outputs the amplitudes. In that case, we can use a phase reconstruction algorithm such as the classic **Griffin-Lim** algorithm, or using a neural network called a vocoder, to reconstruct a waveform from the spectrogram.


### mel spectrogram

Creating a mel spectrogram is a lossy operation as it involves filtering the signal. Converting a mel spectrogram back into a waveform is more difficult than doing this for a regular spectrogram, as it requires estimating the frequencies that were thrown away. This is why machine learning models such as **HiFiGAN vocoder** are needed to produce a waveform from a mel spectrogram.



# Streaming audio data


* **Audio files are large** üìÇ ‚Üí Storing them on disk takes up significant space.
* **Streaming allows real-time processing** ‚è≥ ‚Üí You don‚Äôt need to store the entire dataset; instead, you process chunks on the fly.
* **Trade-off** ‚öñÔ∏è ‚Üí If you don‚Äôt save processed data, you must **recompute** it each time you use it.

---

### üîç **Key Insights on Streaming Audio Data**

1Ô∏è‚É£ **Why Streaming?**

* Saves storage space.
* Allows **real-time** processing for applications like live transcription, voice assistants, and speech recognition.
* Makes it easier to experiment with different datasets without needing huge storage capacity.
* When streaming a large audio dataset, how soon can you start using it? As soon as the first example is downloaded.

2Ô∏è‚É£ **Downsides of Streaming**

* **Reprocessing required** each time you use the data.
* **Latency** (small delay) can occur when fetching and processing data.
* Some **deep learning models** may need all data at once (batch processing), so streaming may not always be ideal.

3Ô∏è‚É£ **When to Use Streaming vs. Storing Locally?**

* ‚úÖ **Use Streaming** when dealing with **large datasets, live audio, or real-time processing** (e.g., ASR, speaker verification).
* ‚úÖ **Store Locally** when **reprocessing is expensive** and you need **quick access** to the same features (e.g., training a model on preprocessed features).
